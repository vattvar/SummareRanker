{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/vv2116/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n",
      "100%|██████████| 3/3 [00:00<00:00, 216.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import get_dataset_split_names\n",
    "from model import ModelMultitaskBinary\n",
    "from moe_vv import *\n",
    "\n",
    "\n",
    "xsum_dataset = load_dataset(\"xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "parser = argparse.ArgumentParser(prog='myprogram', description='Foo')\n",
    "parser.add_argument('--expert_hidden_size', type=int, default=1024)\n",
    "parser.add_argument('--tower_hidden_size', type=int, default=1024)\n",
    "parser.add_argument('--hidden_size', type=int, default=1024) # 768 / 1024\n",
    "parser.add_argument('--bottom_hidden_size', type=int, default=1024)\n",
    "parser.add_argument('--num_experts', type=int, default=6)\n",
    "parser.add_argument('--scoring_methods', type=str, default = [\"rouge_1\",\"rogue_2\",\"rouge_l\"])\n",
    "parser.add_argument('--k', type=int, default=3)\n",
    "\n",
    "parser.add_argument('--max_len', type=int, default=448)\n",
    "parser.add_argument('--max_summ_len', type=int, default=64)\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.n_tasks = len(args.scoring_methods)\n",
    "args.n_positives = 1\n",
    "args.n_negatives = 1\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "args.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'r1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'r1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/candidate_generation/candidates_1.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m df[df\u001b[39m.\u001b[39mcolumns[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m df[df\u001b[39m.\u001b[39mcolumns[\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m arr : arr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mr1\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mr1\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m arr : [\u001b[39mfloat\u001b[39m(val) \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m arr[\u001b[39m2\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mr2\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mr2\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m arr : [\u001b[39mfloat\u001b[39m(val) \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m arr[\u001b[39m2\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mrls\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mrls\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m arr : [\u001b[39mfloat\u001b[39m(val) \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m arr[\u001b[39m2\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'r1'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/candidate_generation/candidates_1.csv\")\n",
    "df[df.columns[1]] = df[df.columns[1]].apply(lambda arr : arr.split('|'))\n",
    "df[\"r1\"] = df[\"r1\"].apply(lambda arr : [float(val) for val in arr[2:-2].split(',')])\n",
    "df[\"r2\"] = df[\"r2\"].apply(lambda arr : [float(val) for val in arr[2:-2].split(',')])\n",
    "df[\"rls\"] = df[\"rls\"].apply(lambda arr : [float(val) for val in arr[2:-2].split(',')])\n",
    "df[\"scores\"] = df.apply(lambda row : [row[\"r1\"],row[\"r2\"],row[\"rls\"]],axis=1)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "df[\"scores\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "from transformers import PegasusForConditionalGeneration,PegasusTokenizer\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "base_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "xsum_dataset = MultitaskRerankingDatasetTrain(\"train\", tokenizer, df[df.columns[0]].tolist(), df[df.columns[1]].tolist(), df[df.columns[2]].tolist(),df[\"scores\"].tolist(), args.max_len,args.max_summ_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from utils import *\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def nested_detach(tensors):\n",
    "        if isinstance(tensors, (list, tuple)):\n",
    "            return type(tensors)(nested_detach(t) for t in tensors)\n",
    "        return tensors.detach()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        mode = inputs[\"mode\"]\n",
    "        text_and_summaries_ids = inputs[\"text_and_summaries_input_ids\"]\n",
    "        text_and_summaries_mask = inputs[\"text_and_summaries_attn_mask\"]\n",
    "        scores = inputs[\"scores\"]\n",
    "\n",
    "        outputs = model(mode, text_and_summaries_ids, text_and_summaries_mask, scores)\n",
    "\n",
    "        loss = outputs[\"loss\"]\n",
    "        output = torch.zeros(2 + 3 * args.n_tasks + 2).float().to(loss.device)\n",
    "        output[0] = loss\n",
    "        output[1] = outputs[\"loss_nce\"]\n",
    "        for j in range(args.n_tasks):\n",
    "            output[2 + j * 3] = outputs[\"accuracy_{}\".format(args.scoring_methods[j])]\n",
    "            output[3 + j * 3] = outputs[\"rank_{}\".format(args.scoring_methods[j])]\n",
    "            output[4 + j * 3] = outputs[\"prediction_{}\".format(args.scoring_methods[j])]\n",
    "        output[-2] = outputs[\"prediction_sum\"]\n",
    "        output[-1] = outputs[\"overall_sum\"]\n",
    "\n",
    "        return (loss, output) if return_outputs else loss\n",
    "\n",
    "    def prediction_step(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "            prediction_loss_only: bool,\n",
    "            ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (:obj:`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "            ignore_keys (:obj:`Lst[str]`, `optional`):\n",
    "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
    "                gathering predictions.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
    "            logits and labels (each being optional).\n",
    "        \"\"\"\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "        if has_labels:\n",
    "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
    "            if len(labels) == 1:\n",
    "                labels = labels[0]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if has_labels:\n",
    "                loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "                loss = loss.mean().detach()\n",
    "                if isinstance(outputs, dict):\n",
    "                    logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                else:\n",
    "                    logits = outputs[1:]\n",
    "            else:\n",
    "                loss = None\n",
    "                if self.use_amp:\n",
    "                    # with autocast():\n",
    "                    outputs = model(**inputs)\n",
    "                else:\n",
    "                    text_inputs_ids = inputs[\"text_inputs_ids\"]\n",
    "                    text_attention_mask = inputs[\"text_attention_mask\"]\n",
    "                    text_inputs = {\n",
    "                        \"input_ids\": text_inputs_ids,\n",
    "                        \"attention_mask\": text_attention_mask\n",
    "                    }\n",
    "                    outputs = model(**text_inputs)\n",
    "                if isinstance(outputs, dict):\n",
    "                    logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                # TODO: this needs to be fixed and made cleaner later.\n",
    "                if self.args.past_index >= 0:\n",
    "                    self._past = outputs[self.args.past_index - 1]\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        logits = nested_detach(logits)\n",
    "        if len(logits) == 1:\n",
    "            logits = logits[0]\n",
    "\n",
    "        return (loss, logits, labels)\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the training :class:`~torch.utils.data.DataLoader`.\n",
    "\n",
    "        Will use no sampler if :obj:`self.train_dataset` does not implement :obj:`__len__`, a random sampler (adapted\n",
    "        to distributed training if necessary) otherwise.\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        # if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "        #     train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.data_collator\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    loss_nce = np.mean([preds[i] for i in range(0, len(preds), 1 + 3 * args.n_tasks + 2)])\n",
    "    result = {\n",
    "        \"loss_nce\": loss_nce\n",
    "    }\n",
    "    for j in range(args.n_tasks):\n",
    "        accuracy_arr = [preds[i] for i in range(1 + j * 3, len(preds), 1 + 3 * args.n_tasks + 2)]\n",
    "        accuracy = np.mean(accuracy_arr)\n",
    "        rank_arr = [preds[i] for i in range(2 + j * 3, len(preds), 1 + 3 * args.n_tasks + 2)]\n",
    "        rank = np.mean(rank_arr)\n",
    "        prediction_arr = [preds[i] for i in range(3 + j * 3, len(preds), 1 + 3 * args.n_tasks + 2)]\n",
    "        prediction = np.mean(prediction_arr)\n",
    "        print(\"Task {}, # pred batches: {}\".format(j + 1, len(accuracy_arr)))\n",
    "        result[\"accuracy_{}\".format(args.scoring_methods[j])] = accuracy\n",
    "        result[\"rank_{}\".format(args.scoring_methods[j])] = rank\n",
    "        result[\"prediction_{}\".format(args.scoring_methods[j])] = prediction\n",
    "    prediction_sum = np.mean([preds[i] for i in range(1 + 3 * args.n_tasks, len(preds), 1 + 3 * args.n_tasks + 2)])\n",
    "    result[\"prediction_sum\"] = prediction_sum\n",
    "    overall_sum = np.mean([preds[i] for i in range(1 + 3 * args.n_tasks + 1, len(preds), 1 + 3 * args.n_tasks + 2)])\n",
    "    result[\"overall_sum\"] = overall_sum\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(xsum_dataset, batch_size = 2, shuffle = False)\n",
    "# x1 = next(iter(train_loader))\n",
    "# # x1 = xsum_dataset.__getitem__(0)\n",
    "# x1\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "# n_metrics = 2\n",
    "# input_size = 3 \n",
    "# output_size=3\n",
    "# num_experts = 6\n",
    "# hidden_size = 4\n",
    "# k=2\n",
    "# moe = MoE(device, n_metrics, input_size, output_size, num_experts, hidden_size, k=4)\n",
    "# x = torch.randn(10,3)\n",
    "# moe(x)\n",
    "\n",
    "# moe = MoE(args.device, args.n_tasks, 5, args.hidden_size, args.num_experts, args.expert_hidden_size, args.k)\n",
    "# x = torch.randn(10,5)\n",
    "# moe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ModelMultitaskBinary\n",
    "from training_utils import *\n",
    "\n",
    "model = ModelMultitaskBinary(base_model, tokenizer, args)\n",
    "# model(x1[\"mode\"],x1[\"text_and_summaries_input_ids\"],x1[\"text_and_summaries_attn_mask\"],x1[\"scores\"])\n",
    "# args.max_train_size = 100\n",
    "# args.max_val_size=50\n",
    "\n",
    "# train_dataset = xsum_dataset\n",
    "# train_dataset.texts = xsum_dataset.texts[:args.max_train_size]\n",
    "# train_dataset.summaries = xsum_dataset.summaries[:args.max_train_size]\n",
    "# train_dataset.labels = xsum_dataset.labels[:args.max_train_size]\n",
    "# train_dataset.scores = xsum_dataset.scores[:args.max_train_size]\n",
    "\n",
    "\n",
    "# val_dataset = xsum_dataset\n",
    "# val_dataset.texts = xsum_dataset.texts[args.max_train_size:args.max_train_size+args.max_val_size]\n",
    "# val_dataset.summaries = xsum_dataset.summaries[args.max_train_size:args.max_train_size+args.max_val_size]\n",
    "# val_dataset.labels = xsum_dataset.labels[args.max_train_size:args.max_train_size+args.max_val_size]\n",
    "# val_dataset.scores = xsum_dataset.scores[args.max_train_size:args.max_train_size+args.max_val_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 170\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 50\n",
      "  Number of trainable parameters = 583434243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "170\n",
      "170\n",
      "170\n",
      "170\n",
      "170\n",
      "170\n",
      "170\n",
      "170\n",
      "170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead\n",
      "  warnings.warn(\"is_namedtuple is deprecated, please use the python checks instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# training loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     model\u001b[39m.\u001b[39mdisplay_training_labels()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2507\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2508\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2511\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[1;32m/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb Cell 9\u001b[0m in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m text_and_summaries_mask \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39mtext_and_summaries_attn_mask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m scores \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39mscores\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(mode, text_and_summaries_ids, text_and_summaries_mask, scores)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bburstcontainer/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/summareranker/main_vv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mn_tasks \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(loss\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m    170\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgather(outputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:183\u001b[0m, in \u001b[0;36mDataParallel.gather\u001b[0;34m(self, outputs, output_device)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather\u001b[39m(\u001b[39mself\u001b[39m, outputs, output_device):\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mreturn\u001b[39;00m gather(outputs, output_device, dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:86\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m# Recursive function calls like this create reference cycles.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m# Setting the function to None clears the refcycle.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     res \u001b[39m=\u001b[39m gather_map(outputs)\n\u001b[1;32m     87\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     gather_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:77\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m outputs):\n\u001b[1;32m     76\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAll dicts must have the same number of keys\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(out)((k, gather_map([d[k] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m outputs]))\n\u001b[1;32m     78\u001b[0m                      \u001b[39mfor\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m out)\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:77\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m outputs):\n\u001b[1;32m     76\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAll dicts must have the same number of keys\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)((k, gather_map([d[k] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m outputs]))\n\u001b[1;32m     78\u001b[0m                      \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m out)\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:81\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(out)(\u001b[39mmap\u001b[39;49m(gather_map, \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49moutputs)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:81\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49moutputs)))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir=\"models/model_1\",  # will be changed\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=False,\n",
    "    num_train_epochs=5,\n",
    "    optim = \"adafactor\",\n",
    "    adafactor=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=5,\n",
    "    learning_rate=1e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    gradient_accumulation_steps = 8,\n",
    "    max_grad_norm=10e5,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    train_dataset=xsum_dataset,\n",
    "    eval_dataset=xsum_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args\n",
    ")\n",
    "\n",
    "\n",
    "# training loop\n",
    "if True:\n",
    "    trainer.train()\n",
    "    model.display_training_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# val_loader = torch.utils.data.DataLoader(xsum_dataset, batch_size = 2, shuffle = False)\n",
    "# for i, batch in tqdm(enumerate(val_loader)):\n",
    "#     print( batch[\"text_and_summaries_input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27960, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/vv2116/1006/cds-bootcamp/homework/nlp/NLP_Final/SummaReranker/src/candidate_generation/candidate_scores_1.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>rl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The full cost of damage in Newton Stewart, one...</td>\n",
       "      <td>The clean-up operation is continuing in parts ...</td>\n",
       "      <td>Clean-up operations are continuing across the ...</td>\n",
       "      <td>[0.5945945978164673, 0.523809552192688, 0.6999...</td>\n",
       "      <td>[0.5945945978164673, 0.523809552192688, 0.6999...</td>\n",
       "      <td>[0.4324324429035187, 0.3333333432674408, 0.550...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fire alarm went off at the Holiday Inn in Ho...</td>\n",
       "      <td>Two tourist buses have been destroyed in a sus...</td>\n",
       "      <td>Two tourist buses have been destroyed by fire ...</td>\n",
       "      <td>[0.800000011920929, 0.9375, 0.8666666746139526...</td>\n",
       "      <td>[0.800000011920929, 0.9375, 0.8666666746139526...</td>\n",
       "      <td>[0.800000011920929, 0.9375, 0.8666666746139526...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ferrari appeared in a position to challenge un...</td>\n",
       "      <td>Lewis Hamilton beat Mercedes team-mate Nico Ro...</td>\n",
       "      <td>Lewis Hamilton stormed to pole position at the...</td>\n",
       "      <td>[0.8823529481887817, 0.8823529481887817, 0.848...</td>\n",
       "      <td>[0.8823529481887817, 0.8823529481887817, 0.848...</td>\n",
       "      <td>[0.5882353186607361, 0.5882353186607361, 0.606...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John Edward Bates, formerly of Spalding, Linco...</td>\n",
       "      <td>A former Lincolnshire Police officer has gone ...</td>\n",
       "      <td>A former Lincolnshire Police officer carried o...</td>\n",
       "      <td>[0.39024388790130615, 0.4324324429035187, 0.38...</td>\n",
       "      <td>[0.39024388790130615, 0.4324324429035187, 0.38...</td>\n",
       "      <td>[0.3414634168148041, 0.37837839126586914, 0.33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patients and staff were evacuated from Cerahpa...</td>\n",
       "      <td>Turkish police have ended a siege at a psychia...</td>\n",
       "      <td>An armed man who locked himself into a room at...</td>\n",
       "      <td>[0.4864864945411682, 0.5714285969734192, 0.565...</td>\n",
       "      <td>[0.4864864945411682, 0.5714285969734192, 0.565...</td>\n",
       "      <td>[0.37837839126586914, 0.523809552192688, 0.478...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  The full cost of damage in Newton Stewart, one...   \n",
       "1  A fire alarm went off at the Holiday Inn in Ho...   \n",
       "2  Ferrari appeared in a position to challenge un...   \n",
       "3  John Edward Bates, formerly of Spalding, Linco...   \n",
       "4  Patients and staff were evacuated from Cerahpa...   \n",
       "\n",
       "                                                   1  \\\n",
       "0  The clean-up operation is continuing in parts ...   \n",
       "1  Two tourist buses have been destroyed in a sus...   \n",
       "2  Lewis Hamilton beat Mercedes team-mate Nico Ro...   \n",
       "3  A former Lincolnshire Police officer has gone ...   \n",
       "4  Turkish police have ended a siege at a psychia...   \n",
       "\n",
       "                                                   2  \\\n",
       "0  Clean-up operations are continuing across the ...   \n",
       "1  Two tourist buses have been destroyed by fire ...   \n",
       "2  Lewis Hamilton stormed to pole position at the...   \n",
       "3  A former Lincolnshire Police officer carried o...   \n",
       "4  An armed man who locked himself into a room at...   \n",
       "\n",
       "                                                  r1  \\\n",
       "0  [0.5945945978164673, 0.523809552192688, 0.6999...   \n",
       "1  [0.800000011920929, 0.9375, 0.8666666746139526...   \n",
       "2  [0.8823529481887817, 0.8823529481887817, 0.848...   \n",
       "3  [0.39024388790130615, 0.4324324429035187, 0.38...   \n",
       "4  [0.4864864945411682, 0.5714285969734192, 0.565...   \n",
       "\n",
       "                                                  r2  \\\n",
       "0  [0.5945945978164673, 0.523809552192688, 0.6999...   \n",
       "1  [0.800000011920929, 0.9375, 0.8666666746139526...   \n",
       "2  [0.8823529481887817, 0.8823529481887817, 0.848...   \n",
       "3  [0.39024388790130615, 0.4324324429035187, 0.38...   \n",
       "4  [0.4864864945411682, 0.5714285969734192, 0.565...   \n",
       "\n",
       "                                                  rl  \n",
       "0  [0.4324324429035187, 0.3333333432674408, 0.550...  \n",
       "1  [0.800000011920929, 0.9375, 0.8666666746139526...  \n",
       "2  [0.5882353186607361, 0.5882353186607361, 0.606...  \n",
       "3  [0.3414634168148041, 0.37837839126586914, 0.33...  \n",
       "4  [0.37837839126586914, 0.523809552192688, 0.478...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
